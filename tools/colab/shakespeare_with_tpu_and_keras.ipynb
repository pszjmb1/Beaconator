{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict Shakespeare with Cloud TPUs and Keras",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bfJp0Q0M1npS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "metadata": {
        "id": "rxkNIKFM1vWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edfbxDDh2AEs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predict Shakespeare with Cloud TPUs and Keras"
      ]
    },
    {
      "metadata": {
        "id": "wYp6XVQU2POn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        " <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "xzpUtDMqmA-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This example uses [tf.keras](https://www.tensorflow.org/guide/keras) to build a *language model* and train it on a [Google Cloud TPU](https://cloud.google.com/tpu/). This language model predicts the next character of text given the text so far. The trained model can generate new snippets of text that read in a similar style to the text training data.\n",
        "\n",
        "We'll train the model on the combined works of William Shakespeare, then use it to compose a play in the style of *The Great Bard*:\n",
        "\n",
        "<blockquote>\n",
        "Loves that led me no dumbs lack her Berjoy's face with her to-day.  \n",
        "The spirits roar'd; which shames which within his powers  \n",
        "\tWhich tied up remedies lending with occasion,  \n",
        "A loud and Lancaster, stabb'd in me  \n",
        "\tUpon my sword for ever: 'Agripo'er, his days let me free.  \n",
        "\tStop it of that word, be so: at Lear,  \n",
        "\tWhen I did profess the hour-stranger for my life,  \n",
        "\tWhen I did sink to be cried how for aught;  \n",
        "\tSome beds which seeks chaste senses prove burning;  \n",
        "But he perforces seen in her eyes so fast;  \n",
        "And _  \n",
        "</blockquote>\n",
        "\n",
        "Note: To enable TPUs on Google Colab, select *Runtime > Change runtime type*, and set *Hardware acceleration* to TPU."
      ]
    },
    {
      "metadata": {
        "id": "KRQ6Fjra3Ruq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download data\n",
        "\n",
        "Download *The Complete Works of William Shakespeare* as a single text file from [Project Gutenberg](https://www.gutenberg.org/). We'll use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
      ]
    },
    {
      "metadata": {
        "id": "j8sIXh1DEDDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3635cec2-ddea-45f8-8d4a-92b747a67c73"
      },
      "cell_type": "code",
      "source": [
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "/content/shakespear 100%[===================>]   5.58M  8.24MB/s    in 0.7s    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AbL6cqCl7hnt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the data generator"
      ]
    },
    {
      "metadata": {
        "id": "E3V4V-Jxmuv3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "f13aed46-6ff4-48e3-ef36-33226197e457"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import six\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "\n",
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def transform(txt, pad_to=None):\n",
        "  # drop any non-ascii characters\n",
        "  output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
        "  if pad_to is not None:\n",
        "    output = output[:pad_to]\n",
        "    output = np.concatenate([\n",
        "        np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
        "        output,\n",
        "    ])\n",
        "  return output\n",
        "\n",
        "def training_generator(seq_len=100, batch_size=1024):\n",
        "  \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
        "  with tf.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
        "    txt = f.read()\n",
        "\n",
        "  tf.logging.info('Input text [%d] %s', len(txt), txt[:50])\n",
        "  source = transform(txt)\n",
        "  while True:\n",
        "    offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
        "\n",
        "    # Our model uses sparse crossentropy loss, but Keras requires labels\n",
        "    # to have the same rank as the input logits.  We add an empty final\n",
        "    # dimension to account for this.\n",
        "    yield (\n",
        "        np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
        "        np.expand_dims(\n",
        "            np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]),\n",
        "            -1),\n",
        "    )\n",
        "\n",
        "six.next(training_generator(seq_len=10, batch_size=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Input text [5834393] ﻿\r\n",
            "Project Gutenberg’s The Complete Works of Willi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[103, 101, 114,  13,  10,  32,  32,  32,  32,  79]], dtype=int32),\n",
              " array([[[101],\n",
              "         [114],\n",
              "         [ 13],\n",
              "         [ 10],\n",
              "         [ 32],\n",
              "         [ 32],\n",
              "         [ 32],\n",
              "         [ 32],\n",
              "         [ 79],\n",
              "         [117]]], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "Bbb05dNynDrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "The model is defined as a two-layer, forward-LSTM—with two changes from the `tf.keras` standard LSTM definition:\n",
        "\n",
        "1. Define the input `shape` of our model which satisfies the [XLA compiler](https://www.tensorflow.org/performance/xla/)'s static shape requirement.\n",
        "2. Use `tf.train.Optimizer` instead of a standard Keras optimizer (Keras optimizer support is still experimental)."
      ]
    },
    {
      "metadata": {
        "id": "yLEM-fLJlEEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
        "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
        "  source = tf.keras.Input(\n",
        "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
        "  model = tf.keras.Model(inputs=[source], outputs=[predicted_char])\n",
        "  model.compile(\n",
        "      optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzBYDJI0_Tfm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "The `tf.contrib.tpu.keras_to_tpu_model` function converts a `tf.keras` model to an equivalent TPU version. We then use the standard Keras methods to train: `fit`, `predict`, and `evaluate`."
      ]
    },
    {
      "metadata": {
        "id": "ExQ922tfzSGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "outputId": "37398fbc-1a3e-4937-847e-48ac9459ee40"
      },
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "training_model = lstm_model(seq_len=100, batch_size=128, stateful=False)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    training_model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.fit_generator(\n",
        "    training_generator(seq_len=100, batch_size=1024),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10,\n",
        ")\n",
        "tpu_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.127.233.10:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14709014610966547359)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7565363478113739515)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 12398459008426899412)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12925220325497171761)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 18279639809709125442)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3840005543040994596)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16971008860836384387)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6102746640990122615)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6514333340752961111)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 2106337854104935670)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7960394206215018774)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9171919761255909345)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: b'grpc://10.127.233.10:8470'\n",
            "Epoch 1/10\n",
            "INFO:tensorflow:Input text [5834393] ﻿\n",
            "Project Gutenberg’s The Complete Works of Willi\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.int32, name='seed0'), TensorSpec(shape=(128, 100, 1), dtype=tf.float32, name='time_distributed_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for seed\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.73502779006958 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "100/100 [==============================] - 25s 246ms/step - loss: 4.5096 - sparse_categorical_accuracy: 0.1863\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 16s 158ms/step - loss: 3.2560 - sparse_categorical_accuracy: 0.2056\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 16s 158ms/step - loss: 2.0342 - sparse_categorical_accuracy: 0.4106\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 16s 158ms/step - loss: 1.4331 - sparse_categorical_accuracy: 0.5670\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 16s 159ms/step - loss: 1.2778 - sparse_categorical_accuracy: 0.6068\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 16s 158ms/step - loss: 1.2134 - sparse_categorical_accuracy: 0.6231\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 16s 158ms/step - loss: 1.1740 - sparse_categorical_accuracy: 0.6339\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 16s 157ms/step - loss: 1.1546 - sparse_categorical_accuracy: 0.6387\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 16s 156ms/step - loss: 1.1331 - sparse_categorical_accuracy: 0.6441\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 16s 157ms/step - loss: 1.1202 - sparse_categorical_accuracy: 0.6476\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TCBtcpZkykSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make predictions with the model\n",
        "\n",
        "Use the trained model to make predictions and generate your own Shakespeare-esque play.\n",
        "Start the model off with a *seed* sentence, then generate 250 characters from it. We'll make five predictions from the initial seed."
      ]
    },
    {
      "metadata": {
        "id": "tU7M-EGGxR3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1dd85c7b-0f36-4ab9-c4ea-da23ec4e0f70"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "\n",
        "# We seed the model with our initial string, copied BATCH_SIZE times\n",
        "\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "  prediction_model.predict(seed[:, i:i + 1])\n",
        "\n",
        "# Now we can accumulate predictions!\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "  \n",
        "  # sample from our output distribution\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  \n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  generated = ''.join([chr(c) for c in p])\n",
        "  print(generated)\n",
        "  print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            " This is my lord; superfluity\r\n",
            "    Than my insolence this ashes arms.\r\n",
            "  LUCIO. Here comes the flower.\r\n",
            "  TIMON. Command it, then, son, by the Duke,\r\n",
            "    Let not his fost strong eyes,\r\n",
            "    Were I the day of all description so great\r\n",
            "    For money-she\n",
            "\n",
            "PREDICTION 1\n",
            "\n",
            "\n",
            " I partly bring him again.\r\n",
            "How much to the most unfill scirit may\r\n",
            "To great favours life reserves it straight!\r\n",
            "Mistrust your mel limitates, and almost\r\n",
            "truly discoursed, and an E-hour Emperor.\r\n",
            "    Great syears in all girdles in this cus!\r\n",
            "I do beg\n",
            "\n",
            "PREDICTION 2\n",
            "\n",
            "\n",
            "              Exeunt\r\n",
            "\r\n",
            "SCENE IV. Another part\r\n",
            "\r\n",
            "Enter ORBESSE Anne, Master Slender,\r\n",
            "    Murders dear Easky, and the mighty spirits are writ\r\n",
            "    And leave thy detesting queas; and birth himself grave, and exercise\r\n",
            "    And yet I'll be his power, m\n",
            "\n",
            "PREDICTION 3\n",
            "\n",
            "\n",
            " I care for offence.\r\n",
            "The grief may do his wife. Pray, sir, as a licence\r\n",
            "Siddled that else died to try reside to the world,\r\n",
            "It hath disobyrward to the strand liege,\r\n",
            "Wilt kill your request;\r\n",
            "Unto my Husband, to my self will please your face; for he\n",
            "\n",
            "PREDICTION 4\n",
            "\n",
            "\n",
            " Lead him to a time;\r\n",
            "    We are consuls, lechery, jot; for you can but deliver it it\r\n",
            "    But of the sudden proper mans stern curses to the self rescue,\r\n",
            "    Besides, the LOVELLOX\r\n",
            "\r\n",
            "  LAUNCELOT. Now, I prithee, my brother's wife;\r\n",
            "    Nothing shall\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "imc3dxJJvTFc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}